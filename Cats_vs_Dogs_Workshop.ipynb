{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cats_vs_Dogs_Workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tijlk/cats_vs_dogs/blob/master/Cats_vs_Dogs_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3e77fe89648ec296a679f08619109c5568339e4d",
        "colab_type": "text",
        "id": "u92hv95mPrkV"
      },
      "source": [
        "# Transfer Learning in image recognition - Cats vs. Dogs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8CWWSGRSmAz",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlW5dNMjSbvI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Cats vs. Dogs\n",
        "\n",
        "Training dataset of 25,000 images (50% cats, 50% dogs).\n",
        "\n",
        "Competition on Kaggle: [Dogs vs. Cats Redux Kernels Edition](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition)\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*R-ItxBW2SWarITBKe7HZuA.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiRFOiabSdOb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## References\n",
        "\n",
        "* Guide on [transfer learning on medium.com](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)\n",
        "* 'Deep Learning with Python' by Francois Chollet ([pdf](http://www.google.com/search?q=deep+learning+with+python+francois+chollet+pdf), [Amazon](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNUD5gruSegT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Setting up Google Colab\n",
        "\n",
        "*   First of all, it's probably a good idea to save this notebook in your Google Drive. To do that, go to File and click on 'Save a copy in Drive'. Otherwise, you might lose your results if you're not careful.\n",
        "*   You want to use GPU's, so make sure to select a GPU runtime. Go to 'Runtime' -> 'Change runtime type'. Select 'GPU' as Hardware accelarator and click on 'Save'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u-kNLB19KrPE"
      },
      "source": [
        "## Setting up the Kaggle API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jrtR-OCGKnd5",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"uniteds\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"e2cc23b4870d3b069e2f8bf9d159847d\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mhRL8GLGKnd8"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "Download the dataset. It contains a `train.zip` with 25,000 images, and a `test.zip` with 12,500 images that you can use to score and submit to the Kaggle competition. We won't use the test.zip file. This should only take about a minute. It's downloading to the Google servers, not to your local drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "khPqX6zrKnd9",
        "colab": {}
      },
      "source": [
        "!kaggle competitions download -c dogs-vs-cats-redux-kernels-edition\n",
        "!unzip -q -o train.zip\n",
        "!unzip -q -o test.zip\n",
        "!chmod 644 *.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mHl9F46sPrkp"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ghSBAc-pPrkq",
        "colab": {}
      },
      "source": [
        "!pip install wget\n",
        "import wget\n",
        "import cv2, glob, keras, os, random, shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "    from google.colab.patches import cv2_imshow\n",
        "except ModuleNotFoundError:\n",
        "    def cv2_imshow(img):\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "    \n",
        "from keras import models, layers, regularizers, optimizers\n",
        "from keras import backend as K\n",
        "from keras.applications import VGG16\n",
        "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
        "from keras.layers import Input, Dropout, Flatten, Convolution2D, Conv2D, \\\n",
        "    MaxPooling2D, Dense, Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import np_utils\n",
        "from matplotlib import ticker\n",
        "from numbers import Number\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "%matplotlib inline "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vRU7gh30Prkt"
      },
      "source": [
        "# Preparing the Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hkQ8fhXUwJDq"
      },
      "source": [
        "## Putting the data into folders\n",
        "\n",
        "The data is now available as `.jpg` images. We need to translate it to floating point numbers in order for Keras to be able to work with it. We will be using the `flow_from_directory` method from Keras for this, which does all the translation from .jpg to integers and then to floating points for us.\n",
        "\n",
        "To be able to use this method, we need to put the files in a certain directory structure as shown below.\n",
        "\n",
        "See also [this tutorial on medium.com](https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720).\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1000/1*HpvpA9pBJXKxaPCl5tKnLg.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0t81HVwJE5LO",
        "colab": {}
      },
      "source": [
        "def gather_training_images(n_cats_train=1000, n_cats_validation=500):\n",
        "    root_dir = ''\n",
        "    original_train_dir = os.path.join(root_dir, 'train')\n",
        "    original_test_dir = os.path.join(root_dir, 'test')\n",
        "    base_dir = os.path.join(root_dir, 'cats_and_dogs_small')\n",
        "\n",
        "    # Directory with our training cat pictures\n",
        "    train_dir = os.path.join(base_dir, 'train')\n",
        "    train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "    train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "    # Directory with our validation cat pictures\n",
        "    validation_dir = os.path.join(base_dir, 'validation')\n",
        "    validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "    validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "    test_base_dir = os.path.join(base_dir, 'test')\n",
        "    test_dir = os.path.join(test_base_dir, 'images')\n",
        "\n",
        "    dirs = [base_dir, train_dir, train_cats_dir, train_dogs_dir, validation_dir, \n",
        "            validation_cats_dir, validation_dogs_dir, test_base_dir, test_dir]\n",
        "    # Clean up the directories if they already exist\n",
        "    for dir in dirs:\n",
        "        try:\n",
        "            shutil.rmtree(dir)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "        os.mkdir(dir)\n",
        "        \n",
        "    cat_train_files = [file for file in os.listdir(original_train_dir) if 'cat' in file]\n",
        "    dog_train_files = [file for file in os.listdir(original_train_dir) if 'dog' in file]\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    train_cats, validation_cats = train_test_split(cat_train_files, train_size=n_cats_train,\n",
        "                                                   test_size=n_cats_validation, random_state=42)\n",
        "    train_dogs, validation_dogs = train_test_split(dog_train_files, train_size=n_cats_train,\n",
        "                                                   test_size=n_cats_validation, random_state=42)\n",
        "\n",
        "    # Copy training and validation images\n",
        "    grps = [{'target_dir': train_cats_dir, 'files': train_cats},\n",
        "            {'target_dir': train_dogs_dir, 'files': train_dogs},\n",
        "            {'target_dir': validation_cats_dir, 'files': validation_cats},\n",
        "            {'target_dir': validation_dogs_dir, 'files': validation_dogs}]\n",
        "    for grp in grps:\n",
        "        for fname in grp['files']:\n",
        "            src = os.path.join(original_train_dir, fname)\n",
        "            dst = os.path.join(grp['target_dir'], fname)\n",
        "            shutil.copyfile(src, dst)\n",
        "\n",
        "    # Copy test images\n",
        "    for fname in [f for f in os.listdir('test')]:\n",
        "        src = os.path.join(os.path.join(root_dir, 'test'), fname)\n",
        "        spl = fname.split('.')\n",
        "        dst = os.path.join(test_dir, f\"{int(spl[0]):05d}.jpg\")\n",
        "        shutil.copyfile(src, dst)\n",
        "        \n",
        "    for dir in ['cats_and_dogs_small/train/cats',\n",
        "                'cats_and_dogs_small/train/dogs',\n",
        "                'cats_and_dogs_small/validation/cats',\n",
        "                'cats_and_dogs_small/validation/dogs',\n",
        "                'cats_and_dogs_small/test/images']:\n",
        "        print(f\"Directory {dir} contains {len(os.listdir(dir))} files.\")\n",
        "    \n",
        "    return train_dir, validation_dir, test_base_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL5RBouaKhIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir, validation_dir, test_base_dir = \\\n",
        "    gather_training_images(n_cats_train=500, n_cats_validation=250)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTC-xjLMF7f",
        "colab_type": "text"
      },
      "source": [
        "## Creating data generators using `flow_from_directory`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZMa7SzDEriF9",
        "colab": {}
      },
      "source": [
        "def get_data_generator(train_dir, validation_dir, test_base_dir, size=150,\n",
        "                       batch_size=20):\n",
        "    \"\"\"Returns a generator for batches of training data and\n",
        "    a generator for batches of validation data.\"\"\"\n",
        "    # All images will be rescaled by 1./255\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "        \n",
        "    # Point the generators to the right folders and define the batch size\n",
        "    # and target size\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "            train_dir,  # This is the target directory\n",
        "            target_size=(size, size),  # All images will be resized to 150x150\n",
        "            batch_size=batch_size,\n",
        "            # Since we use binary_crossentropy loss, we need binary labels\n",
        "            class_mode='binary')\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "            validation_dir,\n",
        "            target_size=(size, size),\n",
        "            batch_size=batch_size,\n",
        "            class_mode='binary')\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "            test_base_dir,\n",
        "            target_size=(size, size),\n",
        "            batch_size=1,\n",
        "            shuffle=False,\n",
        "            class_mode=None)\n",
        "    \n",
        "    for data_batch, labels_batch in train_generator:\n",
        "        print(f'\\ndata batch shape: {data_batch.shape}')\n",
        "        print(f'labels batch shape: {labels_batch.shape}')\n",
        "        break\n",
        "\n",
        "    return train_generator, validation_generator, test_generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnTQH3MwMowa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen, valid_gen, test_gen = get_data_generator(\n",
        "    train_dir, validation_dir, test_base_dir, size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSanxRmZUruC",
        "colab_type": "text"
      },
      "source": [
        "So, the `train_gen` generator yields both a batch of 20 images and a batch of 20 labels. Let's show one batch of images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmuMgftKQCZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for data_batch, labels_batch in train_gen:\n",
        "    size = 100\n",
        "    n_cols, n_rows = (5, 4)\n",
        "    display_grid = np.zeros((size * n_rows, n_cols * size, 3))\n",
        "\n",
        "    # We'll tile each filter into this big horizontal grid\n",
        "    i = 0\n",
        "    for col in range(n_cols):\n",
        "        for row in range(n_rows):\n",
        "            display_grid[row * size : (row + 1) * size,\n",
        "                         col * size : (col + 1) * size, :] = data_batch[i,:]\n",
        "            i += 1\n",
        "\n",
        "    # Display the grid\n",
        "    plt.figure(figsize=(1.5/size * display_grid.shape[1],\n",
        "                        1.5/size * display_grid.shape[0]))\n",
        "    plt.imshow(display_grid)\n",
        "    plt.axis('off')\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kfSmNCvSCerR"
      },
      "source": [
        "# Convolutional neural networks\n",
        "\n",
        "A convolutional neural network consists of two parts: a convolutional part and a classification (or regression) part.\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/dobVrh3SGyqQraM2ogi-P3VK2K-LFsBm7RLO)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfKGFjnvCfWx",
        "colab_type": "text"
      },
      "source": [
        "### Convolution\n",
        "\n",
        "The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information sliding over each other. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map.\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/d0ufdQE7LHA43cdSrVefw2I9DFceYMixqoZJ)\n",
        "\n",
        "The fundamental difference between a dense layer and a convolutional layer is that a dense layer learns global patterns in the input space, whereas a convolutional layer learns local patterns (of size 3x3 or 5x5 pixels usually). This means that:\n",
        "* the patterns a convolutional layer learns are *translation invariant*\n",
        "* multiple convolutionaly layers stacked on top of each other can learn *spatial hierarchies of patterns*. The lower layers learn local patterns such as edges, while upper layers will learn increasingly abstract and complex visual concepts such as ears and eyes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnSZf4nfSti5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Classification\n",
        "\n",
        "The second part of the architecture then takes these higher level *features* and uses them in a couple of fully connected (dense) layers to classify the image as for example a cat or a dog.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiXAu19ed2UP",
        "colab_type": "text"
      },
      "source": [
        "# Training our CNN's\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_-bmpV3j1pa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Our first example network\n",
        "\n",
        "Let's have a look at a very simple but working example of a convolutional neural network that can distinguish cats from dogs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DjRFLWLw-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymFqAngpY8-h",
        "colab_type": "text"
      },
      "source": [
        "Adding the first convolutional layer with 32 filters of 3x3 pixels with **ReLU activation**. Why ReLU? \n",
        "\n",
        "* Easy to compute\n",
        "* Converges faster than sigmoid or tanh because it doesn't suffer from vanishing gradients\n",
        "* Is sparsely activated, which prevents overfitting\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*DfMRHwxY1gyyDmrIAd-gjQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWw-cxA-Y6og",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(size, size, 3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZyU1eDqYdLS",
        "colab_type": "text"
      },
      "source": [
        "-----\n",
        "\n",
        "\n",
        "### QUESTION 1\n",
        "_How many parameters are going to be trained for this first convolutional layer?_\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtMsk-A7XBFl",
        "colab_type": "text"
      },
      "source": [
        "The image below shows how the feature map is constructed from each filter sliding over the input image in the first layer.\n",
        "\n",
        "**Convolution in 3 dimensions**:\n",
        "\n",
        "![](https://m-alcu.github.io/assets/cntk103d_conv2d_final.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGjECLZrZwHh",
        "colab_type": "text"
      },
      "source": [
        "Next, we add a Max Pooling layer. This reduces the dimensionality and prevents overfitting.\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/96HH3r99NwOK818EB9ZdEbVY3zOBOYJE-I8Q)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwmTMZ4TZiJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(layers.MaxPooling2D((2, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZt7djWfZ_Hl",
        "colab_type": "text"
      },
      "source": [
        "Then we add three more convolutional and max pooling layers, gradually decreasing the width and height of the feature maps and increasing the number of filters in each convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZRGJQ-EZ-ML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITBa3BKbbu8v",
        "colab_type": "text"
      },
      "source": [
        "-----\n",
        "\n",
        "### QUESTION 2\n",
        "_How many parameters are to be trained for the final convolutional layer?_\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAdI2t52aZk0",
        "colab_type": "text"
      },
      "source": [
        "Finally, we flatten the final feature map into a vector and connect that to two dense layers which will do the final classification of the image. We're doing binary classification, so we use a sigmoid activation function for the final node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOqC9mydaY3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWa--j7FcET4",
        "colab_type": "text"
      },
      "source": [
        "Finally, you can verify your answers on the two questions using `model.summary()`. The number of parameters for each layer is shown on the right side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UY0dhDNXaKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXQsnPdBXAye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen, validation_gen, test_gen = \\\n",
        "    get_data_generator(train_dir, validation_dir, test_base_dir, size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KAtQyOneauq",
        "colab_type": "text"
      },
      "source": [
        "### Let's train this thing...\n",
        "\n",
        "First we compile the model and specify that we use the RMSprop optimizer, binary cross entropy as a loss function and want to measure the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D20L0CABevR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a-5Wk52fLz-",
        "colab_type": "text"
      },
      "source": [
        "Next we first create some helper functions that will make it easier for us to quickly train new models and show results.\n",
        "\n",
        "* **`train_model`** is a function that trains the model with a given data generator, stores results in a history object and saves the best model to disk as an `.h5` file. Furthermore you can use `early_stopping_patience` to stop the training when the model hasn't improved on the validation set for more than `x` epochs.\n",
        "\n",
        "In Keras, we can implement **early stopping** as a callback function. Callbacks are functions that can be applied at certain stages of the training process, such as at the end of each epoch. Specifically, in our solution, we included `EarlyStopping(monitor='val_loss', patience=15)` to define that we wanted to monitor the test (validation) loss at each epoch and after the test loss has not improved after fifteen epochs, training is interrupted. However, since we set patience=15, we wonâ€™t get the best model, but the model fifteen epochs after the best model. Therefore, optionally, we can include a second operation, `ModelCheckpoint` which saves the model to a file after every checkpoint (which can be useful in case a multi-day training session is interrupted for some reason. Helpful for us, if we set `save_best_only=True` then `ModelCheckpoint` will only save the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dfvONjDiszET",
        "colab": {}
      },
      "source": [
        "def train_model(model, train_generator, validation_generator, n_epochs=30, \n",
        "                filename='cats_and_dogs.h5', early_stopping_patience=None,\n",
        "                batch_size=20):\n",
        "    \"\"\"Fits the model to the training data and monitors accuracy and loss\n",
        "    on the validation set as well\"\"\"\n",
        "    n_train_images = len(os.listdir(train_dir + '/cats')) * 2\n",
        "    print(f\"Training on {n_train_images} training images. Each epoch has \"\n",
        "          f\"{n_train_images // batch_size} batches of {batch_size} images.\")\n",
        "    \n",
        "    # I'm using early stopping to monitor the validation loss and stop the\n",
        "    # training when the validation loss hasn't improved for a number of epochs\n",
        "    # I also keep track of the model with the best validation loss using the\n",
        "    # modelcheckpoint callback.\n",
        "    if early_stopping_patience is not None:\n",
        "        es = EarlyStopping(monitor='val_acc', mode='max', \n",
        "                           verbose=1, patience=early_stopping_patience)\n",
        "        mc = ModelCheckpoint(filename, monitor='val_acc', mode='max',\n",
        "                             verbose=0, save_best_only=True)\n",
        "        callbacks = [es, mc]\n",
        "    else:\n",
        "        callbacks = None\n",
        "        \n",
        "    # Fit the model\n",
        "    history = model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=n_train_images // batch_size,\n",
        "        epochs=n_epochs,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=n_train_images // batch_size // 2,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "    \n",
        "    # Save the model if it wasn't already saved with the modelcheckpoints\n",
        "    if early_stopping_patience is None:\n",
        "        model.save(filename)\n",
        "        \n",
        "    # Return the history so we can plot the results.\n",
        "    return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pWrIenWf2Ld",
        "colab_type": "text"
      },
      "source": [
        "* **`evaluate_model`** shows accuracy and loss values for training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HOzL9Gsbs7qP",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model, train_gen, validation_gen):\n",
        "    \"\"\"Evaluate a model on the training, validation and test sets and\n",
        "    show the accuracy and loss for each\"\"\"\n",
        "    print(\"Best model:\")\n",
        "    # Evaluate the model on each of the datasets\n",
        "    tr_r = model.evaluate_generator(train_gen, steps=len(train_gen))\n",
        "    va_r = model.evaluate_generator(validation_gen, steps=len(validation_gen))\n",
        "    \n",
        "    # See https://blog.henryhhammond.com/pandas-formatting-snippets/\n",
        "    def as_percent(v, precision='0.1'):  \n",
        "        \"\"\"Convert number to percentage string.\"\"\"\n",
        "        if isinstance(v, Number):\n",
        "            return \"{{:{}%}}\".format(precision).format(v)\n",
        "        else:\n",
        "            raise TypeError(\"Numeric type required\")\n",
        "\n",
        "    # Reformat the results into a dataframe\n",
        "    df = pd.DataFrame([tr_r, va_r])\n",
        "    df.index = ['Train', 'Validation']\n",
        "    df.columns = ['Loss', 'Accuracy_float']\n",
        "    df['Accuracy'] = df['Accuracy_float'].apply(as_percent)\n",
        "    df = df[['Loss', 'Accuracy']]\n",
        "    df = df.round({\"Loss\":3})\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idKjnDGpf9N9",
        "colab_type": "text"
      },
      "source": [
        "* **`plot_accuracy`** plots the accuracy and loss of the training and validations sets through the epochs.*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kmFV4LkXs-1C",
        "colab": {}
      },
      "source": [
        "def plot_accuracy(history):\n",
        "    \"\"\"Plots the accuracy and loss of the training and validation sets.\"\"\"\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(len(acc))\n",
        "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJGpVCF2YLZo",
        "colab_type": "text"
      },
      "source": [
        "* **`show_example_predictions`** shows predictions on some sample images, as well as some predictions for which it is unsure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5myVcM9lPSpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_example_predictions(model):\n",
        "    test_gen.reset()\n",
        "    print(\"-\"*60+\"\\nMy prediction on some random cats and dogs:\\n\")\n",
        "    for i in range(0,5):\n",
        "        img = test_gen.next()\n",
        "        pred = model.predict(img)[0]\n",
        "        if pred >= 0.5: \n",
        "            print('I am {:.0%} sure this is a Dog'.format(pred[0]))\n",
        "        else: \n",
        "            print('I am {:.0%} sure this is a Cat'.format(1-pred[0]))\n",
        "        cv2_imshow(img[0]*255)\n",
        "        plt.show()\n",
        "        print(\"\")\n",
        "\n",
        "    i = 1\n",
        "    j = 0\n",
        "    print(\"-\"*60+\"\\nSome images for which I'm not so sure:\\n\")\n",
        "    while i <= 5:\n",
        "        img = test_gen.next()\n",
        "        pred = model.predict(img)[0]\n",
        "        if pred <= 0.8 and pred >= 0.2:\n",
        "            if pred >= 0.5: \n",
        "                print('I am {:.0%} sure this is a Dog'.format(pred[0]))\n",
        "            else: \n",
        "                print('I am {:.0%} sure this is a Cat'.format(1-pred[0]))\n",
        "            cv2_imshow(img[0]*255)\n",
        "            plt.show()\n",
        "            print(\"\")\n",
        "            i += 1\n",
        "        else:\n",
        "            j += 1\n",
        "            if j >= 10000:\n",
        "                break\n",
        "            pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MWSZj-qgH-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = train_model(model, train_gen, validation_gen, n_epochs=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mbdi99UhDOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate_model(model, train_gen, validation_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT4RFJx1g9S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_accuracy(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Gx_ky7YZ_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_example_predictions(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkt9TYIkieFJ",
        "colab_type": "text"
      },
      "source": [
        "Clearly this model completely overfitted on the training set. Let's try to reduce this with **data augmentation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hvGVgKPpPrk8"
      },
      "source": [
        "## Using data augmentation\n",
        "\n",
        "The `ImageDataGenerator` allows us to *augment* each image each time it is shown to the network. So instead of training on the same image of a dog standing on a table each epoch, the image of the dog standing on the table will be slightly different in each epoch.\n",
        "\n",
        "There are a number of augmentations that can be applied, such as:\n",
        "\n",
        "* rotation\n",
        "* width and height shifting\n",
        "* brightness\n",
        "* shear\n",
        "* zoom\n",
        "* horizontal and/or vertical flipping\n",
        "* ...\n",
        "\n",
        "So let's adjust our `get_data_generator` function to make use of this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFkwYPmnQpww",
        "colab_type": "text"
      },
      "source": [
        "-------\n",
        "\n",
        "### EXERCISE 1: Data augmentation\n",
        "1. _Implement data augmentation in the copy of the `get_data_generator` function below so that overfitting in the training process can be reduced. Use Google._\n",
        "2. _Then train the same model as we did before, but using data augmentation and check whether the model now performs better._\n",
        "\n",
        "-------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcri6VR9mweO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_generator(train_dir, validation_dir, test_base_dir, \n",
        "                       use_augmentation=True, size=150, batch_size=20):\n",
        "    \"\"\"Returns a generator for batches of training data and\n",
        "    a generator for batches of validation data.\"\"\"\n",
        "\n",
        "    # Implement data augmentation so that overfitting in the training process \n",
        "    # can be reduced.\n",
        "    \n",
        "    ...\n",
        "        \n",
        "    # Point the generators to the right folders and define the batch size\n",
        "    # and target size\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "            train_dir,  # This is the target directory\n",
        "            target_size=(size, size),  # All images will be resized to 150x150\n",
        "            batch_size=batch_size,\n",
        "            # Since we use binary_crossentropy loss, we need binary labels\n",
        "            class_mode='binary')\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "            validation_dir,\n",
        "            target_size=(size, size),\n",
        "            batch_size=batch_size,\n",
        "            class_mode='binary')\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "            test_base_dir,\n",
        "            target_size=(size, size),\n",
        "            batch_size=1,\n",
        "            shuffle=False,\n",
        "            class_mode=None)\n",
        "    \n",
        "    for data_batch, labels_batch in train_generator:\n",
        "        print(f'\\ndata batch shape: {data_batch.shape}')\n",
        "        print(f'labels batch shape: {labels_batch.shape}')\n",
        "        break\n",
        "\n",
        "    return train_generator, validation_generator, test_generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9EmyJgpkjNk",
        "colab_type": "text"
      },
      "source": [
        "### Training with data augmentation\n",
        "\n",
        "So with this new `get_data_generator` function, we basically run the same code again, being careful to recreate the network from scratch so we don't accidentally start updating from an already trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT4FMJyMkmx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "                        input_shape=(size, size, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uH_alj0k-eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen, validation_gen, test_gen = \\\n",
        "    get_data_generator(train_dir, validation_dir, test_base_dir, size=100)\n",
        "\n",
        "history = train_model(model, train_gen, validation_gen, n_epochs=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQD4wQsYsEK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(evaluate_model(model, train_gen, validation_gen))\n",
        "plot_accuracy(history)\n",
        "show_example_predictions(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SP1f_roHeHO1"
      },
      "source": [
        "# Transfer Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwHaFdEVsAsY",
        "colab_type": "text"
      },
      "source": [
        "## Using transfer learning for feature extraction\n",
        "\n",
        "![](https://miro.medium.com/max/960/1*Ww3AMxZeoiB84GVSRBr4Bw.png)\n",
        "\n",
        "> Transfer learning is the idea of overcoming the isolated learning paradigm and utilizing knowledge acquired for one task to solve related ones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RnDp3uSpeRaJ"
      },
      "source": [
        "### VGG16\n",
        "\n",
        "VGG16 is a convolutional neural network model achieving 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes.\n",
        "\n",
        "It consists of five max pooling layers with two or three convolutional layers before each max pooling layer.\n",
        "\n",
        "![](https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network.jpg)\n",
        "\n",
        "It uses filters of 3x3 pixels with a padding of 1 pixel to keep the spatial resolution the same after convolution.\n",
        "\n",
        "### Replace the dense layers with a new classifier\n",
        "![swapping FC classifiers](https://s3.amazonaws.com/book.keras.io/img/ch5/swapping_fc_classifier.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81jLHJ_ihcWZ",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "\n",
        "### QUESTION 3\n",
        "_How can I input images of 150x150 pixels into this upgraded VGG16 network?_\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-aB4CwlKu_RC",
        "colab": {}
      },
      "source": [
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(224, 224, 3))\n",
        "\n",
        "conv_base.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kmcsEV9hKzhE",
        "colab": {}
      },
      "source": [
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(32, 32, 3))\n",
        "\n",
        "conv_base.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPj6m-lSrwY_",
        "colab_type": "text"
      },
      "source": [
        "14.7 million parameters in both cases. But different feature map sizes of course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fxgJD5POvMqR",
        "colab": {}
      },
      "source": [
        "class DLModel:\n",
        "    def __init__(self, l2=None, verbose=True, size=150,\n",
        "                 use_augmentation=True, weights='imagenet', use_dropout=False):\n",
        "        K.clear_session()\n",
        "        self.verbose = verbose\n",
        "        if l2 is not None:\n",
        "            self.regularizer = regularizers.l2(l2)\n",
        "        else:\n",
        "            self.regularizer = None\n",
        "        self.model = models.Sequential()\n",
        "        self.conv_base = VGG16(weights=weights, include_top=False,\n",
        "                               input_shape=(size, size, 3))\n",
        "        self.model.add(self.conv_base)\n",
        "        self.model.add(layers.Flatten())\n",
        "        if use_dropout:\n",
        "            self.model.add(layers.Dropout(0.5))\n",
        "        self.model.add(layers.Dense(256, activation='relu', \n",
        "                                    kernel_regularizer=self.regularizer))\n",
        "        if use_dropout:\n",
        "            self.model.add(layers.Dropout(0.5))\n",
        "        self.model.add(layers.Dense(1, activation='sigmoid'))\n",
        "        self.train_gen, self.validation_gen, self.test_gen = \\\n",
        "            get_data_generator(train_dir, validation_dir, test_base_dir,\n",
        "                               size=size, use_augmentation=use_augmentation)\n",
        "        self.unfreeze_layers(n=0)\n",
        "        self.history = None\n",
        "    \n",
        "    def unfreeze_layers(self, n=0):\n",
        "        if self.verbose:\n",
        "            print('Current nr. of trainable layers:',\n",
        "                len(self.model.trainable_weights))\n",
        "        for i, layer in enumerate(self.conv_base.layers):\n",
        "            if i < len(self.conv_base.layers) - n:\n",
        "                layer.trainable = False\n",
        "            else:\n",
        "                layer.trainable = True\n",
        "        if self.verbose:\n",
        "            print('Nr. of trainable layers after adjustments:', \n",
        "                len(self.model.trainable_weights))\n",
        "            for layer in self.model.trainable_weights:\n",
        "                print(f\"    {layer}\")\n",
        "                \n",
        "    def compile_model(self, learning_rate=1e-4):\n",
        "        self.model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=optimizers.RMSprop(lr=learning_rate),\n",
        "                  metrics=['acc'])\n",
        "        if self.verbose:\n",
        "            self.model.summary()\n",
        "        \n",
        "    def train(self, early_stopping_patience=15, filename='cats_dogs_model.h5', \n",
        "              n_epochs=100, learning_rate=1e-4):\n",
        "        self.compile_model(learning_rate=learning_rate)\n",
        "        history = train_model(self.model, self.train_gen, self.validation_gen, \n",
        "                              n_epochs=n_epochs, filename=filename,\n",
        "                              early_stopping_patience=early_stopping_patience)\n",
        "        # Reload the best model\n",
        "        self.model = load_model(filename)\n",
        "        self.conv_base = self.model.layers[0]\n",
        "        self.history = history\n",
        "    \n",
        "    def evaluate(self):\n",
        "        display(evaluate_model(self.model, self.train_gen, \n",
        "                               self.validation_gen))\n",
        "        \n",
        "    def plot_accuracy(self):\n",
        "        if self.history:\n",
        "            plot_accuracy(self.history)\n",
        "        else:\n",
        "            print(\"Model hasn't been trained yet!\")\n",
        "\n",
        "    def show_example_predictions(self):\n",
        "        show_example_predictions(self.model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHcy-GK6wSt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = DLModel(use_augmentation=True, size=100)\n",
        "model.train(filename='everything_frozen.h5', n_epochs=15, learning_rate=1e-4)\n",
        "model.evaluate()\n",
        "model.plot_accuracy()\n",
        "model.show_example_predictions()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n59j-BgsRcDx",
        "colab_type": "text"
      },
      "source": [
        "## Transfer learning with fine-tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JbsUGX3-1tUZ"
      },
      "source": [
        "\n",
        "![fine-tuning VGG16](https://s3.amazonaws.com/book.keras.io/img/ch5/vgg16_fine_tuning.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHD-GpK1ser3",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "\n",
        "### EXERCISE 2: Fine-tuning the last convolutional block\n",
        "\n",
        "_Come up with and implement a strategy for fine-tuning the last convolutional block of the VGG16-model using only the DLModel class and its methods. In the interest of time, keep it to max. 15 epochs per training run. You might want to think about the order of training and unfreezing the different layers, using dropout, using different learning rates..._\n",
        "\n",
        "_Save your model (using the `filename=` parameter in the `train` method) as '`4_layers_unfrozen`'._\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfklwWD9fAGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BellfjG1v_lT"
      },
      "source": [
        "# Visualising what networks learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SmByIWYiuBT",
        "colab_type": "text"
      },
      "source": [
        "## Seeing activations of each node"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7nqmHkIZwBpN",
        "colab": {}
      },
      "source": [
        "model = load_model('everything_frozen.h5')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X_2vqMez3f6y",
        "colab": {}
      },
      "source": [
        "img_path = 'train/cat.1700.jpg'\n",
        "\n",
        "# We preprocess the image into a 4D tensor\n",
        "img = image.load_img(img_path, target_size=(100, 100))\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "# Remember that the model was trained on inputs\n",
        "# that were preprocessed in the following way:\n",
        "img_tensor /= 255.\n",
        "\n",
        "# Its shape is (1, 150, 150, 3)\n",
        "print(img_tensor.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hb_NLzT43oki",
        "colab": {}
      },
      "source": [
        "plt.imshow(img_tensor[0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g2HX0Tqj5nZ8",
        "colab": {}
      },
      "source": [
        "# Extracts the outputs of the top 8 layers:\n",
        "layers_to_show = [3,6,10,14,18] # pooling layers\n",
        "layer_outputs = [layer.output for i, layer in enumerate(model.layers[0].layers) if i in layers_to_show]\n",
        "# Creates a model that will return these outputs, given the model input:\n",
        "activation_model = models.Model(inputs=model.layers[0].get_input_at(0), outputs=layer_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7RbuGaEt5v9D",
        "colab": {}
      },
      "source": [
        "# This will return a list of 5 Numpy arrays:\n",
        "# one array per layer activation\n",
        "activations = activation_model.predict(img_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_bjAl9KF7zAi",
        "colab": {}
      },
      "source": [
        "first_layer_activation = activations[0]\n",
        "plt.matshow(first_layer_activation[0, :, :, 3], cmap='viridis')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CxS-Kobo71YT",
        "colab": {}
      },
      "source": [
        "plt.matshow(first_layer_activation[0, :, :, 30], cmap='viridis')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NVpxzRqv744P",
        "colab": {}
      },
      "source": [
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = []\n",
        "for layer in [model.layers[0].layers[i] for i in layers_to_show]:\n",
        "    layer_names.append(layer.name)\n",
        "\n",
        "images_per_row = 16\n",
        "\n",
        "# Now let's display our feature maps\n",
        "for layer_name, layer_activation in zip(layer_names, activations):\n",
        "    # This is the number of features in the feature map\n",
        "    n_features = layer_activation.shape[-1]\n",
        "\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = layer_activation.shape[1]\n",
        "\n",
        "    # We will tile the activation channels in this matrix\n",
        "    n_cols = np.min([n_features // images_per_row, 4])\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "\n",
        "    # We'll tile each filter into this big horizontal grid\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[0,\n",
        "                                             :, :,\n",
        "                                             col * images_per_row + row]\n",
        "            # Post-process the feature to make it visually palatable\n",
        "            channel_image -= channel_image.mean()\n",
        "            channel_image /= channel_image.std()\n",
        "            channel_image *= 64\n",
        "            channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
        "            display_grid[col * size : (col + 1) * size,\n",
        "                         row * size : (row + 1) * size] = channel_image\n",
        "\n",
        "    # Display the grid\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6gsBzwwOFCX",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "\n",
        "### QUESTION 4\n",
        "\n",
        "_Why are in the top layers some nodes 'blank'? What does this mean?_\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p_hCZ5eizDE",
        "colab_type": "text"
      },
      "source": [
        "## Visualising what a given node has learnt to detect\n",
        "\n",
        "We start with a picture containing random pixels. We apply the network in evaluation mode to this random image, calculate the average activation of a certain feature map in a certain layer from which we then compute the gradients with respect to the input image pixel values. Knowing the gradients for the pixel values we then proceed to update the pixel values in a way that maximizes the average activation of the chosen feature map.\n",
        "\n",
        "Basically we try to find an image that maximizes the average activation of a certain feature map by performing gradient descent optimization on the pixel values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK1Sf94vkjbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlO2vW72kmUa",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "\n",
        "### EXERCISE 3: Filter visualisation\n",
        "\n",
        "_Complete the generate pattern function, so that it initializes a random noise image and through gradient descent optimizes the image such that it activates a given filter as much as possible. Parts missing:_\n",
        "1. _Initialize the random input image_\n",
        "2. _Calculate the gradient_\n",
        "3. _Normalize the gradient_\n",
        "4. _Adjust the input image accordingly_\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "837DX03XQoB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_pattern(model, layer_name, filter_index, size=150, steps=40, vgg_layer=True):\n",
        "    # Initialize the image as a floating point 4D-numpy array filled with \n",
        "    # random 'grey' noise (in the 0. to 255 range). Note that once again, we\n",
        "    # need 4 dimensions rather than 3.\n",
        "    input_img_data = ...\n",
        "\n",
        "    # Build a loss function that maximizes the activation\n",
        "    # of the nth filter of the layer considered.\n",
        "    if vgg_layer:\n",
        "        layer_output = model.layers[0].get_layer(layer_name).output\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "        input_layer = model.layers[0].get_input_at(0)\n",
        "    else:\n",
        "        layer_output = model.get_layer(layer_name).output\n",
        "        loss = K.mean(layer_output[:, filter_index])\n",
        "        input_layer = model.input\n",
        "\n",
        "    # Compute the gradients of the input picture with respect to the loss.\n",
        "    # Use K.gradients for this. Note that the network assumes you're working\n",
        "    # with 4D-tensors, where the first dimension is the number of images.\n",
        "    # We only care about the gradients of the first 'image'\n",
        "    grads = ...\n",
        "\n",
        "    # Normalize the gradients to prevent gradients from exploding and remain\n",
        "    # stable.\n",
        "    normalized_grads = ...\n",
        "\n",
        "    # This function returns the loss and grads given the input picture\n",
        "    iterate = K.function([input_layer], [loss, normalized_grads])\n",
        "    \n",
        "    # Run gradient ascent for 40 steps\n",
        "    for i in range(steps):\n",
        "        loss_value, grads_value = iterate([input_img_data])\n",
        "        \n",
        "        # Adjust the input image based on the gradients\n",
        "        input_img_data = ...\n",
        "    \n",
        "    img = input_img_data[0]\n",
        "\n",
        "    return deprocess_image(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IjxBiJH3_T23",
        "colab": {}
      },
      "source": [
        "def show_best_activations(model, size=100):\n",
        "    for layer_name in tqdm_notebook(\n",
        "            ['block1_conv2', 'block2_conv2', 'block3_conv3', \n",
        "             'block4_conv3', 'block5_conv3', 'dense_1']):\n",
        "        margin = 5\n",
        "        rows = 2\n",
        "        columns = 8\n",
        "\n",
        "        # This a empty (black) image where we will store our results.\n",
        "        results = np.zeros((rows * size + (rows-1) * margin, columns * size + (columns-1) * margin, 3))\n",
        "        print(f\"Layer {layer_name}\")\n",
        "        for i in tqdm_notebook(range(columns)):  # iterate over the rows of our results grid\n",
        "            for j in range(rows):  # iterate over the columns of our results grid\n",
        "                #print(f\"Generating image for filter {j + (i * grid_size)} out of in layer {layer_name}...\")\n",
        "                # Generate the pattern for filter `i + (j * 8)` in `layer_name`\n",
        "                if layer_name == 'dense_1':\n",
        "                    filter_img = generate_pattern(model, layer_name, j + (i * rows), size=size, vgg_layer=False)\n",
        "                else:\n",
        "                    filter_img = generate_pattern(model, layer_name, j + (i * rows), size=size, vgg_layer=True)\n",
        "\n",
        "                # Put the result in the square `(i, j)` of the results grid\n",
        "                horizontal_start = i * size + i * margin\n",
        "                horizontal_end = horizontal_start + size\n",
        "                vertical_start = j * size + j * margin\n",
        "                vertical_end = vertical_start + size\n",
        "                results[vertical_start: vertical_end, horizontal_start: horizontal_end, :] = filter_img\n",
        "\n",
        "        # Display the results grid\n",
        "        plt.figure(figsize=(16, 4))\n",
        "        plt.imshow(results.astype(int))\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac4Z0fIEpHid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('everything_frozen.h5')\n",
        "show_best_activations(model, size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJA-5nz0Juqv",
        "colab_type": "text"
      },
      "source": [
        "### More advanced version\n",
        "\n",
        "[Guide on medium.com](https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030)\n",
        "\n",
        "Some impressive looking filters:\n",
        "\n",
        "![](https://miro.medium.com/max/5176/1*8C49dLMgINot63DvafDD3g.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xJ5iBfunBvng"
      },
      "source": [
        "## Visualizing heatmaps of class activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-WKf2qfq3gl",
        "colab_type": "text"
      },
      "source": [
        "This allows you to debug why an image was classified the way it was. Can be very useful to detect problems with the model or the data, as in this case where a horse was detected based on a watermark, rather than the horse itself:\n",
        "\n",
        "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQaoXmQIdO7Ogv-Cm4ekVtuRMDnHKFg1-4uejqrjylhm5iV4kf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ8YvspASHbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('4_layers_unfrozen.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jwR0VFLSDTqE",
        "colab": {}
      },
      "source": [
        "def load_img_to_tensor(img_path, size=150):\n",
        "    # We preprocess the image into a 4D tensor\n",
        "    img = image.load_img(img_path, target_size=(size, size))\n",
        "    img_tensor = image.img_to_array(img)\n",
        "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "\n",
        "    # Remember that the model was trained on inputs\n",
        "    # that were preprocessed in the following way:\n",
        "    img_tensor /= 255.\n",
        "    \n",
        "    return img_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "un95ZSAk4hz_",
        "colab": {}
      },
      "source": [
        "def create_heatmap(img, model, best_class, n_filters=512):\n",
        "    # This is the \"african elephant\" entry in the prediction vector\n",
        "    object_output = model.layers[0].get_output_at(0)[:, best_class]\n",
        "\n",
        "    # The is the output feature map of the `block5_conv3` layer,\n",
        "    # the last convolutional layer in VGG16\n",
        "    last_conv_layer_output = model.layers[0].get_layer('block5_conv3').output\n",
        "\n",
        "    # This is the gradient of the \"african elephant\" class with regard to\n",
        "    # the output feature map of `block5_conv3`\n",
        "    grads = K.gradients(object_output, last_conv_layer_output)[0]\n",
        "\n",
        "    # This is a vector of shape (512,), where each entry\n",
        "    # is the mean intensity of the gradient over a specific feature map channel\n",
        "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # This function allows us to access the values of the quantities we just defined:\n",
        "    # `pooled_grads` and the output feature map of `block5_conv3`,\n",
        "    # given a sample image\n",
        "    iterate = K.function([model.layers[0].get_input_at(0)], \n",
        "                         [pooled_grads, last_conv_layer_output[0]])\n",
        "\n",
        "    # These are the values of these two quantities, as Numpy arrays,\n",
        "    # given our sample image of two elephants\n",
        "    pooled_grads_value, conv_layer_output_value = iterate([img])\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the elephant class\n",
        "    for i in range(n_filters):\n",
        "        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
        "\n",
        "    # The channel-wise mean of the resulting feature map\n",
        "    # is our heatmap of class activation\n",
        "    heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap /= np.max(heatmap)\n",
        "    return heatmap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1g2B-QjlCrt2",
        "colab": {}
      },
      "source": [
        "def show_heatmap(heatmap):\n",
        "    plt.matshow(heatmap)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UFuCYdFUCvCX",
        "colab": {}
      },
      "source": [
        "def superimpose(img_path, heatmap, intensity=0.4):\n",
        "    # We use cv2 to load the original image\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # We resize the heatmap to have the same size as the original image\n",
        "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]), interpolation = cv2.INTER_NEAREST)\n",
        "\n",
        "    # We convert the heatmap to RGB\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # We apply the heatmap to the original image\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "    # 0.4 here is a heatmap intensity factor\n",
        "    superimposed_img = heatmap * intensity + img\n",
        "\n",
        "    vis = np.concatenate((img, superimposed_img), axis=1)\n",
        "    width = 600\n",
        "    height = int(width / vis.shape[1] * vis.shape[0])\n",
        "    \n",
        "    vis_resized = cv2.resize(vis, (width, height))\n",
        "\n",
        "    # Save the image to disk\n",
        "    cv2_imshow(vis_resized)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-GcvXnEyCbZb",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# Take some random images in the training set\n",
        "imgs = os.listdir('train')\n",
        "np.random.shuffle(imgs)\n",
        "\n",
        "for img_file in imgs[:10]:\n",
        "    # Load an image into a tensor\n",
        "    img_path = f'train/{img_file}'\n",
        "    img_tensor = load_img_to_tensor(img_path)\n",
        "\n",
        "    # predict whether it is a cat or a dog\n",
        "    pred = model.predict(img_tensor)[0]\n",
        "    if pred >= 0.5: \n",
        "        print(f'\\nI am {pred[0]:.0%} sure this is a Dog')\n",
        "    else: \n",
        "        print(f'\\nI am {1-pred[0]:.0%} sure this is a Cat')\n",
        "    \n",
        "    # Create and superimpose the heatmap\n",
        "    heatmap = create_heatmap(img_tensor, model, 0)\n",
        "    superimpose(img_path, heatmap, intensity=0.6)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxtTivpPkvfo",
        "colab_type": "text"
      },
      "source": [
        "## Everything combined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hh2gKiEmiv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_max_activation(model, img_url, size=200):\n",
        "\n",
        "    # Extracts the outputs of the top 8 layers:\n",
        "    layer_outputs = model.layers[0].layers[17].output\n",
        "    # Creates a model that will return these outputs, given the model input:\n",
        "    activation_model = models.Model(inputs=model.layers[0].get_input_at(0), outputs=layer_outputs)\n",
        "    # We preprocess the image into a 4D tensor\n",
        "    try:\n",
        "        img_path = wget.download(img_url)\n",
        "    except:\n",
        "        print(\"ERROR - Please try a different image\")\n",
        "        return\n",
        "    img = image.load_img(img_path, target_size=(size, size))\n",
        "    img_tensor = image.img_to_array(img)\n",
        "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "    # Remember that the model was trained on inputs\n",
        "    # that were preprocessed in the following way:\n",
        "    img_tensor /= 255.\n",
        "\n",
        "    activations = activation_model.predict(img_tensor)\n",
        "    mean_act = np.mean(activations,axis=(0,1,2))\n",
        "    tuples = [(i, v) for i, v in enumerate(list(mean_act))]\n",
        "    tuples = sorted(tuples, key = lambda x: x[1], reverse=True)\n",
        "    plt.figure(figsize=(7,5))\n",
        "    act = plt.plot(mean_act,linewidth=2.)\n",
        "    most_activated_filters = [i for i, v in tuples[:5]]\n",
        "    print(f\"Most activated filters are {most_activated_filters}\")\n",
        "    extraticks=most_activated_filters\n",
        "    ax = act[0].axes\n",
        "    ax.set_xlim(0,500)\n",
        "    for filter in most_activated_filters:\n",
        "        plt.axvline(x=filter, color='grey', linestyle='--')\n",
        "    ax.set_xlabel(\"feature map\")\n",
        "    ax.set_ylabel(\"mean activation\")\n",
        "    ax.set_xticks([0,200,400] + extraticks)\n",
        "    plt.show()\n",
        "\n",
        "    rows = 1\n",
        "    columns = 5\n",
        "    margin = 5\n",
        "    results = np.zeros((rows * size + (rows-1) * margin, columns * size + (columns-1) * margin, 3))\n",
        "    for i, good_filter in enumerate(most_activated_filters):\n",
        "        filter_img = generate_pattern(model, 'block5_conv3', good_filter, size=size, steps=100)\n",
        "        horizontal_start = i * size + i * margin\n",
        "        horizontal_end = horizontal_start + size\n",
        "        results[0: size, horizontal_start: horizontal_end, :] = filter_img\n",
        "    plt.figure(figsize=(15, 3))\n",
        "    plt.imshow(results.astype(int))\n",
        "    plt.show()\n",
        "\n",
        "    # predict whether it is a cat or a dog\n",
        "    pred = model.predict(img_tensor)[0]\n",
        "    if pred >= 0.5: \n",
        "        print(f'\\nI am {pred[0]:.0%} sure this is a Dog')\n",
        "    else: \n",
        "        print(f'\\nI am {1-pred[0]:.0%} sure this is a Cat')\n",
        "    \n",
        "    # Create and superimpose the heatmap\n",
        "    heatmap = create_heatmap(img_tensor, model, 0)\n",
        "    superimpose(img_path, heatmap, intensity=0.6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U53td1kiqUuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('4_layers_unfrozen.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23Sk2FKIk35x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_max_activation(model, 'https://pbs.twimg.com/profile_images/1080545769034108928/CEzHCTpI_400x400.jpg', size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yRaNQvTk81K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_max_activation(model, 'https://cdn130.picsart.com/288118701068211.png', size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWLe-jXElBWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_max_activation(model, 'https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/dogs-that-dont-shed-1560974761.jpg', size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpif2ZH7tqLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_max_activation(model, 'https://www.quartoknows.com/blog/wp-content/uploads/2017/08/Bengal.png', size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPed2TluttY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_max_activation(model, 'https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/kitten-playing-with-toy-mouse-royalty-free-image-590055188-1542816918.jpg', size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD05TaxiuwX1",
        "colab_type": "text"
      },
      "source": [
        "-----\n",
        "\n",
        "### QUESTION 5\n",
        "_Can you find other pictures online that yield interesting activations?_\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdZqIUnru0iF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}